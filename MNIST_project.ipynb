{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8db3483",
   "metadata": {},
   "source": [
    "### OCI Data Science - Useful Tips\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import ADSData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1c7dcec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install boto3\n",
    "!pip install ads\n",
    "!pip install awscli\n",
    "!pip install pytorch-lightning==1.5.10\n",
    "!pip install ipython[notebook]\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install tensorboard pandas\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bd4834b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.11.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision) (2.25.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (6.2.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "da425574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource(\n",
    " 's3',\n",
    " region_name=\"us-phoenix-1\",\n",
    " aws_secret_access_key=\"PcYW3zZugZdfGMk6CQr6OiFy5Cyt8pvctUIQd6NJyns=\",\n",
    " aws_access_key_id=\"e1f1af4d0d5f6fc28bd45aeaec79c8f7554f6410\",\n",
    " endpoint_url=\"https://axutkjfnpof3.compat.objectstorage.us-phoenix-1.oraclecloud.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "12d71884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy, F1Score, Precision, Recall, PrecisionRecallCurve\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "\n",
    "\n",
    "from packaging import version\n",
    "from statistics import mean\n",
    "from packaging import version\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import tensorboard as tb\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import json\n",
    "from tensorboard.plugins.hparams.plugin_data_pb2 import HParamsPluginData\n",
    "\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# Write the version number for the project\n",
    "VERSION_NO = \"v1\"\n",
    "\n",
    "# Folder for storing tensorbolard logs and checkpoints\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model_tensorboard\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fa6c4d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self, data_dir=PATH_DATASETS, hidden_size=64, learning_rate=2e-4):\n",
    "\n",
    "        super().__init__()\n",
    "        # Set our init args as class attributes\n",
    "        self.data_dir = data_dir\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Save the hyperparameters \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Hardcode some dataset specific attributes\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define PyTorch model\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels * width * height, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, self.num_classes),\n",
    "        )\n",
    "        self.accuracy = Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred=self.forward(x)\n",
    "\n",
    "        # identifying number of correct predections in a given batch\n",
    "        correct=pred.argmax(dim=1).eq(y).sum().item()\n",
    "\n",
    "        # identifying total number of labels in a given batch\n",
    "        total=len(y)\n",
    "\n",
    "        #calculating the loss\n",
    "        loss = F.nll_loss(pred, y)\n",
    "        train_loss ={\"train_loss\": loss.item()}\n",
    "\n",
    "        f1= F1Score(self.num_classes, threshold=0.5, average='micro')\n",
    "        f1_score = f1(pred, y)\n",
    "\n",
    "        pre = Precision(self.num_classes)\n",
    "        precisionValue = pre(pred, y)\n",
    "\n",
    "        re = Recall(self.num_classes)\n",
    "        recallValue= re(pred, y)  \n",
    "\n",
    "        output ={\n",
    "            \"loss\": loss,\n",
    "            \"correct\": correct,\n",
    "            \"total\": total,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"precision\":precisionValue,\n",
    "            \"recall\":recallValue\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "       #  the function is called after every epoch is completed\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        avg_precision = torch.stack([x['precision'] for x in outputs]).mean()\n",
    "        avg_recall = torch.stack([x['recall'] for x in outputs]).mean()\n",
    "        avg_f1_score = torch.stack([x['f1_score'] for x in outputs]).mean()\n",
    "\n",
    "        #calculate correct and total predictions\n",
    "        correct=sum([x[\"correct\"] for  x in outputs])\n",
    "        total=sum([x[\"total\"] for  x in outputs])\n",
    "\n",
    "\n",
    "        self.logger.experiment.add_scalar(\"train_loss\", avg_loss, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"train_acc\", correct/total, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"train_recall\", avg_recall, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"train_precision\", avg_precision, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"train_f1_score\", avg_f1_score, self.current_epoch)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        val_acc = self.accuracy(preds, y)\n",
    "\n",
    "        output={'val_loss':loss,\n",
    "                'val_acc' : val_acc\n",
    "                }\n",
    "        return output\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "\n",
    "        self.logger.experiment.add_scalar(\"val_loss\", avg_loss, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"val_accuracy\", avg_acc, self.current_epoch)\n",
    "\n",
    "        return {'val_loss': avg_loss,\n",
    "                'val_accuracy': avg_acc}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        correct=pred.argmax(dim=1).eq(y).sum().item()\n",
    "        total=len(y)*1.0\n",
    "        test_loss = F.nll_loss(pred, y)\n",
    "        test_accuracy = correct/total\n",
    "\n",
    "        f1= F1Score(self.num_classes, threshold=0.5, average='micro')\n",
    "        f1_score = f1(pred, y)\n",
    "\n",
    "        pre = Precision(self.num_classes)\n",
    "        precisionValue = pre(pred, y)\n",
    "\n",
    "        re = Recall(self.num_classes)\n",
    "        recallValue= re(pred, y)  \n",
    "\n",
    "        test_output ={\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_f1\": f1_score,\n",
    "            \"test_recall\": recallValue,\n",
    "            \"test_precision\": precisionValue,\n",
    "            \"correct\": correct,\n",
    "            \"total\": total,\n",
    "        }\n",
    "\n",
    "        return test_output\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        correct=sum([x[\"correct\"] for  x in outputs])\n",
    "        total=sum([x[\"total\"] for  x in outputs])\n",
    "\n",
    "        avg_test_f1_score = torch.stack([x['test_f1'] for x in outputs]).mean()\n",
    "        avg_test_recall = torch.stack([x['test_recall'] for x in outputs]).mean()\n",
    "        avg_test_precision = torch.stack([x['test_precision'] for x in outputs]).mean()\n",
    "\n",
    "\n",
    "        logs = {\"test_loss\": avg_loss, \n",
    "                \"test_accuracy\": correct/total, \n",
    "                \"test_f1_score\":avg_test_f1_score, \n",
    "                \"test_recall\": avg_test_recall, \n",
    "                \"test_precision\": avg_test_precision}\n",
    "\n",
    "        self.logger.experiment.add_scalar(\"test_loss\", avg_loss, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"test_accuracy\", correct/total, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"test_f1_score\", avg_test_f1_score, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"test_recall\", avg_test_recall, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"test_precision\", avg_test_precision, self.current_epoch)\n",
    "\n",
    "        return {'log': logs, 'progress_bar': logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def custom_histogram_adder(self):\n",
    "      for name,params in self.named_parameters():\n",
    "        self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=BATCH_SIZE)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "662b8288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | model    | Sequential | 55.1 K\n",
      "1 | accuracy | Accuracy   | 0     \n",
      "----------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93dbdd8ee42749fb9fcbea2cd6fcdd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LitMNIST()\n",
    "\n",
    "# Define the lightning trainer\n",
    "trainer = Trainer(\n",
    "    gpus=AVAIL_GPUS,\n",
    "    max_epochs=2,\n",
    "    progress_bar_refresh_rate=20, logger=logger,\n",
    ")\n",
    "trainer.tune(model)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9ce747bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at tb_logs/my_model_tensorboard/version_4/checkpoints/epoch=1-step=1720.ckpt\n",
      "Loaded model weights from checkpoint at tb_logs/my_model_tensorboard/version_4/checkpoints/epoch=1-step=1720.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499e545792f949c08755e8d156ff2c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "76db405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | model    | Sequential | 55.1 K\n",
      "1 | accuracy | Accuracy   | 0     \n",
      "----------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally lets train our AI model\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9b8dcb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version_0  version_1  version_2  version_3  version_4\n"
     ]
    }
   ],
   "source": [
    "! ls tb_logs/my_model_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "478643c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-a63b7b36011a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mversions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tb_logs/my_model_tensorboard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mversions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     versions.sort(key=lambda version_string : list(\n\u001b[0m\u001b[1;32m     14\u001b[0m     map(int, re.findall(r'\\d+', version_string)))[0])\n\u001b[1;32m     15\u001b[0m     \u001b[0mlatest_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mversions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-a63b7b36011a>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(version_string)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mversions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     versions.sort(key=lambda version_string : list(\n\u001b[0;32m---> 14\u001b[0;31m     map(int, re.findall(r'\\d+', version_string)))[0])\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mlatest_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mversions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import os\n",
    "\n",
    "# creating the log folder to store json log file of each AI model version\n",
    "\n",
    "os.makedirs(\"formattedjson\", exist_ok=True)\n",
    "os.makedirs(\"finallogfile\", exist_ok=True)\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "\n",
    "versions = os.listdir('tb_logs/my_model_tensorboard')\n",
    "if versions:\n",
    "    versions.sort(key=lambda version_string : list(\n",
    "    map(int, re.findall(r'\\d+', version_string)))[0])\n",
    "    latest_version = versions[-1]\n",
    "\n",
    "print(latest_version)\n",
    "\n",
    "if not os.path.isdir(f\"formattedjson/{latest_version}\"):\n",
    "  os.mkdir(f\"formattedjson/{latest_version}\")\n",
    "\n",
    "if not os.path.isdir(f\"formattedjson/{latest_version}/train_log\"):\n",
    "  os.mkdir(f\"formattedjson/{latest_version}/train_log\")   \n",
    "\n",
    "if not os.path.isdir(f\"formattedjson/{latest_version}/test_log\"):\n",
    "  os.mkdir(f\"formattedjson/{latest_version}/test_log\")\n",
    "\n",
    "if not os.path.isdir(f\"formattedjson/{latest_version}/metrics\"):\n",
    "  os.mkdir(f\"formattedjson/{latest_version}/metrics\")   \n",
    "\n",
    "if not os.path.isdir(f\"finallogfile/{latest_version}\"):\n",
    "  os.mkdir(f\"finallogfile/{latest_version}\") \n",
    "\n",
    "if not os.path.isdir(f\"model/{latest_version}\"):\n",
    "  os.mkdir(f\"model/{latest_version}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "94383a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to write json log file in individual model version\n",
    "def record_log(latest_version):\n",
    "  ea = event_accumulator.EventAccumulator(f'tb_logs/my_model_tensorboard/{latest_version}', size_guidance={event_accumulator.SCALARS: 0})\n",
    "  ea.Reload()\n",
    "  \n",
    "  # saving each logs into json log file\n",
    "  eventTags = ea.Tags()\n",
    "  scalars_var = eventTags[\"scalars\"]\n",
    "  print(f'scalar: ', scalars_var)\n",
    "\n",
    "\n",
    "\n",
    "  # exporting the logs file in json file inside formattedjson folder\n",
    "  for i in scalars_var:\n",
    "    if \"train\" in i or \"val\" in i:\n",
    "      folder_name = \"train_log\"\n",
    "    elif \"test\" in i:\n",
    "      folder_name = \"test_log\"\n",
    "    else:\n",
    "      folder_name = \"metrics\"\n",
    "\n",
    "    # Removing the / with _ while saving logs file , if any logfile name contains any\n",
    "    file_path = f\"formattedjson/{latest_version}/{folder_name}/{i.replace('/', '_')}.json\"\n",
    "\n",
    "    # Converting pandas dataframe into json file format\n",
    "    pd.DataFrame(ea.Scalars(i)).to_json(file_path, orient = 'records')\n",
    "\n",
    "\n",
    "    # Extracting the hyperparameter from scalar to json file\n",
    "  data = ea._plugin_to_tag_to_content[\"hparams\"][\"_hparams_/session_start_info\"]\n",
    "  hparam_data = HParamsPluginData.FromString(data).session_start_info.hparams\n",
    "  hparam_dict = {key: hparam_data[key].ListFields()[0][1] for key in hparam_data.keys()}\n",
    "\n",
    "  with open(f\"formattedjson/{latest_version}/metrics/hparams.json\", \"w\") as outfile:\n",
    "    json.dump(hparam_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "85609ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar:  ['hp_metric', 'val_loss', 'val_accuracy', 'train_loss', 'train_acc', 'train_recall', 'train_precision', 'train_f1_score', 'test_loss', 'test_accuracy', 'test_f1_score', 'test_recall', 'test_precision', 'Valid', 'Train Loss', 'Train Accuracy', 'Train Recall', 'Train Precision', 'Train F1 Score']\n",
      "test_accuracy.json\n",
      "test_f1_score.json\n",
      "test_loss.json\n",
      "test_precision.json\n",
      "test_recall.json\n",
      "val_loss.json\n",
      "train_loss.json\n",
      "train_acc.json\n",
      "train_f1_score.json\n",
      "train_precision.json\n",
      "train_recall.json\n",
      "val_accuracy.json\n"
     ]
    }
   ],
   "source": [
    "# function for combining logs into final single log file\n",
    "def record_final_log(latest_version):\n",
    "  res = []\n",
    "  exclude_dir = set(['metrics'])\n",
    "\n",
    "  dir_path = f'formattedjson/{latest_version}'\n",
    "  # print(dir_path)\n",
    "  for parent_path, dirs, filenames in os.walk(dir_path):\n",
    "    dirs[:] = [d for d in dirs if d not in exclude_dir]\n",
    "    for f in filenames:\n",
    "      print(f)\n",
    "      res.append(os.path.join(parent_path, f))\n",
    "      # print(os.path.join(parent_path, f))\n",
    "\n",
    "  epochs ={}\n",
    "  test_epochs = {}\n",
    "\n",
    "  # print(res)\n",
    "  for metrics_path in res:\n",
    "    epochs_values = pd.read_json(metrics_path)\n",
    "\n",
    "\n",
    "    for index, row in epochs_values.iterrows():\n",
    "      metric_split_name = metrics_path.split(\"/\")\n",
    "      splited_text = os.path.splitext(metric_split_name[-1])[0]\n",
    "\n",
    "      if \"test\" in splited_text:\n",
    "        _epoch_value = test_epochs.get(row['step'], {})\n",
    "\n",
    "      else:\n",
    "        _epoch_value = epochs.get(row['step'], {})\n",
    "\n",
    "      _value = _epoch_value.get('epochs_values', None)\n",
    "      final_value = row['value']\n",
    "      if _value:\n",
    "        final_value = mean([final_value, _value])\n",
    "\n",
    "      # _file_name = os.path.splitext(metrics_path)[0] \n",
    "      _epoch_value.update({splited_text: final_value})\n",
    "\n",
    "      if \"test\" in splited_text:\n",
    "        test_epochs.update({row['step']: _epoch_value})\n",
    "      else:\n",
    "        epochs.update({row['step']: _epoch_value})\n",
    "\n",
    "  final_epochs={}\n",
    "  final_test_epochs = {}\n",
    "\n",
    "  for index, value in enumerate(epochs):\n",
    "    final_epochs.update({index: epochs[value]})\n",
    "\n",
    "  for index, value in enumerate(test_epochs):\n",
    "    final_test_epochs.update(test_epochs[value])  \n",
    "\n",
    "  hparam_file = open(f'formattedjson/{latest_version}/metrics/hparams.json')\n",
    "  hparams_dict = json.load(hparam_file)\n",
    "    \n",
    "  # Date time of project log\n",
    "  log_date = datetime.now()\n",
    "  formatted_log_date = log_date.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "  # Create experiment number from versions of, tensorboard experiment version\n",
    "  version_number = latest_version.split(\"_\")\n",
    "  testver = version_number[-1]\n",
    "  testno = int(testver)\n",
    "#   testno = int(testver) +1    \n",
    "\n",
    "\n",
    "  final_data = [{\n",
    "      \"exp\": {\n",
    "          \"exp_no\": 'exp_'+ str(testno),\n",
    "          \"datetime\": formatted_log_date,\n",
    "          \"hyperparameters\": hparams_dict,\n",
    "          \"epochs\": final_epochs,\n",
    "          \"test_metrics\":final_test_epochs\n",
    "      }\n",
    "      \n",
    "  }]\n",
    "\n",
    "  with open(f\"finallogfile/{latest_version}/log_exp_{testno}.json\", \"w\") as outfile:\n",
    "    json.dump(final_data, outfile) \n",
    "\n",
    "record_log(latest_version)\n",
    "record_final_log(latest_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_bucket(latest_version):\n",
    "    # creating experiment number from latest_version\n",
    "    version_number = latest_version.split(\"_\")\n",
    "    testver = version_number[-1]\n",
    "    testno = int(testver)  \n",
    "    \n",
    "    json_path = f\"finallogfile/{latest_version}/log_exp_{testno}.json\"\n",
    "    print(json_path)\n",
    "    \n",
    "    split_name = json_path.split(\"/\")\n",
    "    splited_text = split_name[-1]\n",
    "    print(splited_text)\n",
    "\n",
    "    # Upload a File to you OCI Bucket, 1st value is the path of the directory, 2nd value is bucket name and 3rd value is file name with version foldername\n",
    "    s3.meta.client.upload_file(json_path, 'ds-1', 'TSD/'+VERSION_NO+'/logs/'+splited_text)\n",
    "    \n",
    "upload_file_bucket(latest_version)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a9a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_model(latest_version):\n",
    "    \n",
    "    # creating experiment number from latest_version\n",
    "    version_number = latest_version.split(\"_\")\n",
    "    testver = version_number[-1]\n",
    "    testno = int(testver)\n",
    "#         testno = int(testver) +1  \n",
    "\n",
    "    \n",
    "    # save the model to local disk\n",
    "    model_name = f'model/{latest_version}/tsd_model_{testno}.pkl'\n",
    "    with open(model_name, 'wb') as files:\n",
    "      pickle.dump(model, files)\n",
    "    \n",
    "    \n",
    "    json_path = f\"model/{latest_version}/tsd_model_{testno}.pkl\"\n",
    "    print(json_path)\n",
    "    split_name = json_path.split(\"/\")\n",
    "    splited_text = split_name[-1]\n",
    "    print(splited_text)\n",
    "    \n",
    "    \n",
    "    # Upload a File to you OCI Bucket, 1st value is the path of the directory, 2nd value is bucket name and 3rd value is file name with version foldername\n",
    "    s3.meta.client.upload_file(json_path, 'ds-1', 'TSD/'+VERSION_NO+'/artifacts/model/'+splited_text)\n",
    "    \n",
    "upload_model(latest_version)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed18f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680e41b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
